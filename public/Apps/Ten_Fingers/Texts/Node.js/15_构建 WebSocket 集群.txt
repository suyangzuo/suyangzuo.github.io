When a single socket server can no longer stomach the millionth concurrent connection, horizontal scaling becomes inevitable. Node.js shines here, but WebSocket statefulness adds spice: every message must reach the correct node that holds the subscriber. Start by swapping the default in-memory adapter for a Redis pub/sub transport via socket.io-redis; one line of code and broadcasts fan out across cores. Next, pin user sessions with consistent hashing on user-id, ensuring socket reconnect lands on the same pod and avoids awkward dual-channel ghosts. Enable sticky sessions on the ingress controller, yet prepare for fail-over: store the full socket room roster in Redis hashes with TTL, so a crashed pod's passengers can be rehydrated elsewhere. Monitor event loop lag per container; when it creeps above forty milliseconds, shed new connections with a 502 so the load balancer diverts traffic to healthier replicas. Finally, run chaos tests: randomly kill pods, flood rooms with synthetic messages, and measure end-to-end latency. If ninety-ninth percentile stays below two hundred milliseconds, you have baked a WebSocket cluster ready for prime-time trivia or crypto ticker storms.