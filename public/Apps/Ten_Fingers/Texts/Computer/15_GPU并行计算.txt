GPU最初为像素而生，却因“单指令多线程”架构成为通用并行计算霸主。一颗消费级显卡集成数千核心，把任务切成32线程一组的"warp"，同一时钟周期内所有线程执行相同指令，只是数据不同——这正是SIMT（Single Instruction, Multiple Threads）精髓。CUDA或OpenCL把线程组织成grid-block-thread三级结构：一个grid可以包含百万线程，硬件调度器动态分发到SM（Streaming Multiprocessor），每个SM用寄存器文件、共享内存、L1缓存喂饱众核心，延迟被海量线程掩盖，吞吐量因此秒杀CPU。内存带宽是生命线，HBM2e堆叠式显存提供TB级带宽，但全局内存访问若未对齐合并，带宽会瞬间蒸发；共享内存按bank划分，冲突访问会让速度掉一个数量级——所以"tile-based"算法先把数据块搬进共享内存，计算完再写回，实现带宽与容量的手动缓存管理。并行规约、扫描、直方图经典模式里，线程协作、避免分支 divergence、减少原子操作竞争，是写kernel时的“三板斧”。深度学习框架把卷积、矩阵乘自动拆成GEMM，再用Tensor Core的混合精度FMAD指令进一步翻倍算力；而实时渲染里，mesh shader、光线追踪BVH遍历也借助GPU并行加速，把“光栅化+RT Core”混合 pipeline 推向影视级画面。今天，用PyTorch写一行.to('cuda')就能调动数千核心，但要想把带宽利用率跑到90%，仍需理解occupancy、latency hiding、shared memory bank conflict——GPU并行计算不是“魔法”，而是“把算法画成线程图，把数据搬成合并访问”的严谨艺术。
